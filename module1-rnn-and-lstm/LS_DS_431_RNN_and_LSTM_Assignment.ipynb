{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "nteract": {
      "version": "0.23.3"
    },
    "colab": {
      "name": "Copia de LS_DS_431_RNN_and_LSTM_Assignment.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsraei20/DS-Unit-4-Sprint-3-Deep-Learning/blob/main/module1-rnn-and-lstm/LS_DS_431_RNN_and_LSTM_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdQNtmPiO16o",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-06-15T18:18:20.442Z",
          "iopub.execute_input": "2020-06-15T18:18:20.453Z",
          "iopub.status.idle": "2020-06-15T18:18:20.513Z",
          "shell.execute_reply": "2020-06-15T18:18:20.523Z"
        },
        "id": "8SGucdttO16o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import pandas as pd"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-06-15T18:25:49.778Z",
          "iopub.execute_input": "2020-06-15T18:25:49.781Z",
          "iopub.status.idle": "2020-06-15T18:25:51.467Z",
          "shell.execute_reply": "2020-06-15T18:25:51.469Z"
        },
        "id": "KTUyYiK-O16r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
        "\n",
        "r = requests.get(url)\n",
        "r.encoding = r.apparent_encoding\n",
        "data = r.text\n",
        "data = data.split('\\r\\n')\n",
        "toc = [l.strip() for l in data[44:130:2]]\n",
        "# Skip the Table of Contents\n",
        "data = data[135:]\n",
        "\n",
        "# Fixing Titles\n",
        "toc[9] = 'THE LIFE OF KING HENRY V'\n",
        "toc[18] = 'MACBETH'\n",
        "toc[24] = 'OTHELLO, THE MOOR OF VENICE'\n",
        "toc[34] = 'TWELFTH NIGHT: OR, WHAT YOU WILL'\n",
        "\n",
        "locations = {id_:{'title':title, 'start':-99} for id_,title in enumerate(toc)}\n",
        "\n",
        "# Start \n",
        "for e,i in enumerate(data):\n",
        "    for t,title in enumerate(toc):\n",
        "        if title in i:\n",
        "            locations[t].update({'start':e})\n",
        "            \n",
        "\n",
        "df_toc = pd.DataFrame.from_dict(locations, orient='index')\n",
        "df_toc['end'] = df_toc['start'].shift(-1).apply(lambda x: x-1)\n",
        "df_toc.loc[42, 'end'] = len(data)\n",
        "df_toc['end'] = df_toc['end'].astype('int')\n",
        "\n",
        "df_toc['text'] = df_toc.apply(lambda x: '\\r\\n'.join(data[ x['start'] : int(x['end']) ]), axis=1)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ6cJFJr1QNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(df):\n",
        "  if df == '':\n",
        "    return np.NaN    \n",
        "  df = df.lower()\n",
        "  df.replace('*',\"\")\n",
        "  df.replace('@',\"\")\n",
        "  df.replace('\\\\',\"\")\n",
        "  df.replace('_',\"\")\n",
        "  return df\n",
        "\n",
        "df_toc['text'] = df_toc['text'].apply(clean_text)\n",
        "df_toc.dropna(inplace=True)\n",
        "df_toc.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHSsDpH26SlG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d95818ea-2d4c-44fb-f15e-ece291280d67"
      },
      "source": [
        "#Shakespeare Data Parsed by Play\n",
        "df_toc.head()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AS YOU LIKE IT</td>\n",
              "      <td>14380</td>\n",
              "      <td>17171</td>\n",
              "      <td>as you like it\\r\\n\\r\\n\\r\\ndramatis personae.\\r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>THE COMEDY OF ERRORS</td>\n",
              "      <td>17172</td>\n",
              "      <td>20372</td>\n",
              "      <td>the comedy of errors\\r\\n\\r\\n\\r\\n\\r\\ncontents\\r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>THE TRAGEDY OF CORIOLANUS</td>\n",
              "      <td>20373</td>\n",
              "      <td>30346</td>\n",
              "      <td>the tragedy of coriolanus\\r\\n\\r\\ndramatis pers...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CYMBELINE</td>\n",
              "      <td>30347</td>\n",
              "      <td>30364</td>\n",
              "      <td>cymbeline.\\r\\nlaud we the gods;\\r\\nand let our...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>THE TRAGEDY OF HAMLET, PRINCE OF DENMARK</td>\n",
              "      <td>30365</td>\n",
              "      <td>37051</td>\n",
              "      <td>the tragedy of hamlet, prince of denmark\\r\\n\\r...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      title  ...                                               text\n",
              "0                            AS YOU LIKE IT  ...  as you like it\\r\\n\\r\\n\\r\\ndramatis personae.\\r...\n",
              "1                      THE COMEDY OF ERRORS  ...  the comedy of errors\\r\\n\\r\\n\\r\\n\\r\\ncontents\\r...\n",
              "2                 THE TRAGEDY OF CORIOLANUS  ...  the tragedy of coriolanus\\r\\n\\r\\ndramatis pers...\n",
              "3                                 CYMBELINE  ...  cymbeline.\\r\\nlaud we the gods;\\r\\nand let our...\n",
              "4  THE TRAGEDY OF HAMLET, PRINCE OF DENMARK  ...  the tragedy of hamlet, prince of denmark\\r\\n\\r...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SatOyaCSoYLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Joins all rows together as a big chunk of text\n",
        "text = \" \".join(df_toc['text'])\n",
        "\n",
        "# filters to have only unique characters \n",
        "chars = list(set(text))\n",
        "\n",
        "# Lookup Tables that give a unique identifier to each unique character\n",
        "char_int = {c:i for i, c in enumerate(chars)} \n",
        "int_char = {i:c for i, c in enumerate(chars)} "
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOYzCBPgp1m7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70db53e9-2581-4c84-b4f4-3e7bf0599b44"
      },
      "source": [
        "# Create the sequence data\n",
        "\n",
        "\n",
        "#parameters of the max lenght and step of the range\n",
        "maxlen = 150\n",
        "step = 50\n",
        "\n",
        "# giving a number value to each character being passed\n",
        "encoded = [char_int[c] for c in text]\n",
        "\n",
        "sequences = [] # Each element is 40 chars long\n",
        "next_char = [] # One element for each sequence\n",
        "\n",
        "for i in range(0, len(encoded) - maxlen, step):\n",
        "    \n",
        "    sequences.append(encoded[i : i + maxlen])\n",
        "    next_char.append(encoded[i + maxlen])\n",
        "    \n",
        "print('sequences: ', len(sequences))\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequences:  304114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYdylmcFtMIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRmLQ4LttFLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, char in enumerate(sequence):\n",
        "        x[i,t,char] = 1\n",
        "        \n",
        "    y[i, next_char[i]] = 1"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brKl4yxb4JxU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "65202850-4f82-4dbf-e903-9c06aa237afd"
      },
      "source": [
        "char_int"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\t': 26,\n",
              " '\\n': 44,\n",
              " '\\r': 25,\n",
              " ' ': 35,\n",
              " '!': 8,\n",
              " '\"': 59,\n",
              " '$': 73,\n",
              " '%': 11,\n",
              " '&': 6,\n",
              " \"'\": 27,\n",
              " '(': 77,\n",
              " ')': 18,\n",
              " '*': 72,\n",
              " ',': 15,\n",
              " '-': 61,\n",
              " '.': 45,\n",
              " '/': 54,\n",
              " '0': 41,\n",
              " '1': 28,\n",
              " '2': 63,\n",
              " '3': 9,\n",
              " '4': 64,\n",
              " '5': 19,\n",
              " '6': 3,\n",
              " '7': 34,\n",
              " '8': 32,\n",
              " '9': 10,\n",
              " ':': 5,\n",
              " ';': 36,\n",
              " '?': 66,\n",
              " '@': 23,\n",
              " '[': 30,\n",
              " '\\\\': 69,\n",
              " ']': 2,\n",
              " '_': 47,\n",
              " '`': 21,\n",
              " 'a': 76,\n",
              " 'b': 58,\n",
              " 'c': 39,\n",
              " 'd': 51,\n",
              " 'e': 57,\n",
              " 'f': 56,\n",
              " 'g': 16,\n",
              " 'h': 12,\n",
              " 'i': 65,\n",
              " 'j': 1,\n",
              " 'k': 75,\n",
              " 'l': 0,\n",
              " 'm': 42,\n",
              " 'n': 55,\n",
              " 'o': 43,\n",
              " 'p': 20,\n",
              " 'q': 71,\n",
              " 'r': 33,\n",
              " 's': 37,\n",
              " 't': 14,\n",
              " 'u': 74,\n",
              " 'v': 53,\n",
              " 'w': 7,\n",
              " 'x': 70,\n",
              " 'y': 29,\n",
              " 'z': 67,\n",
              " '|': 50,\n",
              " '}': 52,\n",
              " 'à': 24,\n",
              " 'â': 22,\n",
              " 'æ': 31,\n",
              " 'ç': 60,\n",
              " 'è': 68,\n",
              " 'é': 46,\n",
              " 'ê': 49,\n",
              " 'î': 62,\n",
              " 'œ': 4,\n",
              " '—': 17,\n",
              " '‘': 38,\n",
              " '’': 13,\n",
              " '“': 40,\n",
              " '”': 48}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vjJ5ZDAuk1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-vjJixStpPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BijRX7wxu_Wg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / 1\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXv-gl3tvCGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    \n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    \n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    \n",
        "    generated = ''\n",
        "    \n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    \n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "    \n",
        "    for i in range(400):\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_int[char]] = 1\n",
        "            \n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds)\n",
        "        next_char = int_char[next_index]\n",
        "        \n",
        "        sentence = sentence[1:] + next_char\n",
        "        \n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print()\n",
        "\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXsDyKcVvE8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bbe04a96-d3c6-433c-80f3-7f4f272c5391"
      },
      "source": [
        "model.fit(x, y,\n",
        "          batch_size=32,\n",
        "          epochs=10,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "9500/9504 [============================>.] - ETA: 0s - loss: 2.1802\n",
            "----- Generating text after Epoch: 0\n",
            "----- Generating with seed: \"hou hear?—most bloody.\n",
            "\n",
            "iago.\n",
            "that’s not amiss.\n",
            "but yet keep time in all. will you withdraw?\n",
            "\n",
            " [_othello withdraws._]\n",
            "\n",
            "now will i question cas\"\n",
            "hou hear?—most bloody.\n",
            "\n",
            "iago.\n",
            "that’s not amiss.\n",
            "but yet keep time in all. will you withdraw?\n",
            "\n",
            " [_othello withdraws._]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "9504/9504 [==============================] - 103s 11ms/step - loss: 2.1801\n",
            "Epoch 2/10\n",
            "9499/9504 [============================>.] - ETA: 0s - loss: 1.8595\n",
            "----- Generating text after Epoch: 1\n",
            "----- Generating with seed: \"the worst best. but what praise\n",
            "couldst thou bestow on a deserving woman indeed, one that in the\n",
            "authority of her merit did justly put on the vouch \"\n",
            "the worst best. but what praise\n",
            "couldst thou bestow on a deserving woman indeed, one that in the\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  drouch the have this spllay hensle'g'd spolk\n",
            "9504/9504 [==============================] - 102s 11ms/step - loss: 1.8595\n",
            "Epoch 3/10\n",
            "9500/9504 [============================>.] - ETA: 0s - loss: 1.7324\n",
            "----- Generating text after Epoch: 2\n",
            "----- Generating with seed: \"s in me\n",
            "thoughts that would thick my blood.\n",
            "\n",
            "leontes.\n",
            "so stands this squire\n",
            "offic’d with me. we two will walk, my lord,\n",
            "and leave you to your gr\"\n",
            "s in me\n",
            "thoughts that would thick my blood.\n",
            "\n",
            "leontes.\n",
            "so stands this squire\n",
            "offic’d with me. we two will walk, my lord,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "by so foul tell., work, pule, lied, \n",
            "9504/9504 [==============================] - 102s 11ms/step - loss: 1.7323\n",
            "Epoch 4/10\n",
            "9501/9504 [============================>.] - ETA: 0s - loss: 1.6558\n",
            "----- Generating text after Epoch: 3\n",
            "----- Generating with seed: \"ank berowne;\n",
            "    the numbers true, and, were the numb'ring too,\n",
            "    i were the fairest goddess on the ground.\n",
            "    i am compar'd to twenty thousand \"\n",
            "ank berowne;\n",
            "    the numbers true, and, were the numb'ring too,\n",
            "    i were the fairest goddess on the ground.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    soory giva\n",
            "9504/9504 [==============================] - 100s 11ms/step - loss: 1.6558\n",
            "Epoch 5/10\n",
            "9503/9504 [============================>.] - ETA: 0s - loss: 1.6025\n",
            "----- Generating text after Epoch: 4\n",
            "----- Generating with seed: \"own’d,\n",
            "stephano? i hope now thou are not drown’d. is the storm overblown? i\n",
            "hid me under the dead moon-calf’s gaberdine for fear of the storm. and\n",
            "\"\n",
            "own’d,\n",
            "stephano? i hope now thou are not drown’d. is the storm overblown? i\n",
            "hid me under the dead moon-calf’s gaberdine for fear of the storm. and\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  kmu\n",
            "9504/9504 [==============================] - 100s 11ms/step - loss: 1.6024\n",
            "Epoch 6/10\n",
            "9504/9504 [==============================] - ETA: 0s - loss: 1.5616\n",
            "----- Generating text after Epoch: 5\n",
            "----- Generating with seed: \"get clear of all the debts i owe.\n",
            "\n",
            "antonio.\n",
            "i pray you, good bassanio, let me know it;\n",
            "and if it stand, as you yourself still do,\n",
            "within the eye \"\n",
            "get clear of all the debts i owe.\n",
            "\n",
            "antonio.\n",
            "i pray you, good bassanio, let me know it;\n",
            "and if it stand, as you yourself still do,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "no my sauble them gomoness in hour off\n",
            "9504/9504 [==============================] - 103s 11ms/step - loss: 1.5616\n",
            "Epoch 7/10\n",
            "9503/9504 [============================>.] - ETA: 0s - loss: 1.5280\n",
            "----- Generating text after Epoch: 6\n",
            "----- Generating with seed: \"\n",
            "\n",
            "olivia.\n",
            "even what it please my lord that shall become him.\n",
            "\n",
            "duke.\n",
            "why should i not, had i the heart to do it,\n",
            "like to the egyptian thief at po\"\n",
            "\n",
            "\n",
            "olivia.\n",
            "even what it please my lord that shall become him.\n",
            "\n",
            "duke.\n",
            "why should i not, had i the heart to do it,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "i will not but hand have\n",
            "9504/9504 [==============================] - 100s 11ms/step - loss: 1.5280\n",
            "Epoch 8/10\n",
            "9503/9504 [============================>.] - ETA: 0s - loss: 1.5007\n",
            "----- Generating text after Epoch: 7\n",
            "----- Generating with seed: \"ect\n",
            "gutenberg are removed. of course, we hope that you will support the\n",
            "project gutenberg-tm mission of promoting free access to electronic\n",
            "works b\"\n",
            "ect\n",
            "gutenberg are removed. of course, we hope that you will support the\n",
            "project gutenberg-tm mission of promoting free access to electronic\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "i thinken sorren be gues's to juig'd to em\n",
            "9504/9504 [==============================] - 100s 10ms/step - loss: 1.5007\n",
            "Epoch 9/10\n",
            "9500/9504 [============================>.] - ETA: 0s - loss: 1.4770\n",
            "----- Generating text after Epoch: 8\n",
            "----- Generating with seed: \"nd the sea mocks\n",
            "our frustrate search on land. well, let him go.\n",
            "\n",
            "antonio.\n",
            "[_aside to sebastian._] i am right glad that he’s\n",
            "so out of hope.\n",
            "do \"\n",
            "nd the sea mocks\n",
            "our frustrate search on land. well, let him go.\n",
            "\n",
            "antonio.\n",
            "[_aside to sebastian._] i am right glad that he’s\n",
            "so out of hope.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "i’ll the dea\n",
            "9504/9504 [==============================] - 103s 11ms/step - loss: 1.4771\n",
            "Epoch 10/10\n",
            "9504/9504 [==============================] - ETA: 0s - loss: 1.4798\n",
            "----- Generating text after Epoch: 9\n",
            "----- Generating with seed: \"se hipped with an old mothy saddle and stirrups of no kindred;\n",
            "besides, possessed with the glanders and like to mose in the chine;\n",
            "troubled with the\"\n",
            "se hipped with an old mothy saddle and stirrups of no kindred;\n",
            "besides, possessed with the glanders and like to mose in the chine;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  laryce. were like we muthil'd it, syell the mannt \n",
            "9504/9504 [==============================] - 91s 10ms/step - loss: 1.4798\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f30f01855f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## Stretch goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
      ]
    }
  ]
}